{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12865219449703091065\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3667263488\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8961232588596411506\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import scipy as sp\n",
    "import PIL\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras import models, layers, Model\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, ZeroPadding2D\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import EfficientNetB4, EfficientNetB6, ResNet50V2\n",
    "#from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size: 한번에 forward & Backword 하는 샘플의 수\n",
    "batch_size = 32\n",
    "\n",
    "# Training 수\n",
    "epochs = 50\n",
    "\n",
    "# Weight 조절 parameter\n",
    "LearningRate = 1e-3 # 0.001\n",
    "Decay = 1e-6\n",
    "\n",
    "img_width = 224\n",
    "img_height = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리 경로 설정 필요\n",
    "CurrentDirectory = \"../\"\n",
    "\n",
    "train_directory = CurrentDirectory + 'data/TRAIN/'\n",
    "test_directory  = CurrentDirectory + 'data/TEST/'\n",
    "model_directory = CurrentDirectory + 'MODEL/'\n",
    "tensorboard_directory = CurrentDirectory + 'Tensorboard'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\y0010\\Desktop\\develop\\Cancer_project\\notebooks\\PYS_classification.ipynb 셀 9\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/y0010/Desktop/develop/Cancer_project/notebooks/PYS_classification.ipynb#ch0000007?line=2'>3</a>\u001b[0m ImageSize \u001b[39m=\u001b[39m (\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/y0010/Desktop/develop/Cancer_project/notebooks/PYS_classification.ipynb#ch0000007?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m FileName \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(train_directory \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBenign/\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/y0010/Desktop/develop/Cancer_project/notebooks/PYS_classification.ipynb#ch0000007?line=4'>5</a>\u001b[0m     train_x\u001b[39m.\u001b[39mappend(cv2\u001b[39m.\u001b[39;49mimread(train_directory \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mBenign/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m FileName))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/y0010/Desktop/develop/Cancer_project/notebooks/PYS_classification.ipynb#ch0000007?line=5'>6</a>\u001b[0m     train_y\u001b[39m.\u001b[39mappend(\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/y0010/Desktop/develop/Cancer_project/notebooks/PYS_classification.ipynb#ch0000007?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m FileName \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(train_directory \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCancer/\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "ImageSize = (224, 224)\n",
    "for FileName in os.listdir(train_directory +'Benign/'):\n",
    "    train_x.append(cv2.imread(train_directory + 'Benign/' + FileName))\n",
    "    train_y.append(0)\n",
    "    \n",
    "for FileName in os.listdir(train_directory +'Cancer/'):\n",
    "    train_x.append(cv2.imread(train_directory +'Cancer/' + FileName))\n",
    "    train_y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>RECENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B_028-0120-1.3.jpg</td>\n",
       "      <td>479</td>\n",
       "      <td>308</td>\n",
       "      <td>248</td>\n",
       "      <td>224</td>\n",
       "      <td>85150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B_003-2017-1.1.jpg</td>\n",
       "      <td>341</td>\n",
       "      <td>381</td>\n",
       "      <td>394</td>\n",
       "      <td>333</td>\n",
       "      <td>85151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B_019-0025-1.0.jpg</td>\n",
       "      <td>492</td>\n",
       "      <td>272</td>\n",
       "      <td>204</td>\n",
       "      <td>190</td>\n",
       "      <td>85152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B_002-4194-1.5.jpg</td>\n",
       "      <td>636</td>\n",
       "      <td>318</td>\n",
       "      <td>157</td>\n",
       "      <td>157</td>\n",
       "      <td>85153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B_013-0124-2.1.jpg</td>\n",
       "      <td>393</td>\n",
       "      <td>307</td>\n",
       "      <td>214</td>\n",
       "      <td>193</td>\n",
       "      <td>85154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13156</th>\n",
       "      <td>M_002-4901-1.1.jpg</td>\n",
       "      <td>205</td>\n",
       "      <td>255</td>\n",
       "      <td>590</td>\n",
       "      <td>468</td>\n",
       "      <td>98306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13157</th>\n",
       "      <td>B_002-0555-1.2.jpg</td>\n",
       "      <td>206</td>\n",
       "      <td>170</td>\n",
       "      <td>309</td>\n",
       "      <td>307</td>\n",
       "      <td>98307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13158</th>\n",
       "      <td>B_002-5254-1.2.jpg</td>\n",
       "      <td>375</td>\n",
       "      <td>395</td>\n",
       "      <td>269</td>\n",
       "      <td>265</td>\n",
       "      <td>98308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13159</th>\n",
       "      <td>B_026-0011-2.0.jpg</td>\n",
       "      <td>387</td>\n",
       "      <td>321</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>98309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13160</th>\n",
       "      <td>B_002-4903-1.3.jpg</td>\n",
       "      <td>427</td>\n",
       "      <td>327</td>\n",
       "      <td>303</td>\n",
       "      <td>314</td>\n",
       "      <td>98310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13161 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 FILENAME    x    y  width  height  RECENT\n",
       "0      B_028-0120-1.3.jpg  479  308    248     224   85150\n",
       "1      B_003-2017-1.1.jpg  341  381    394     333   85151\n",
       "2      B_019-0025-1.0.jpg  492  272    204     190   85152\n",
       "3      B_002-4194-1.5.jpg  636  318    157     157   85153\n",
       "4      B_013-0124-2.1.jpg  393  307    214     193   85154\n",
       "...                   ...  ...  ...    ...     ...     ...\n",
       "13156  M_002-4901-1.1.jpg  205  255    590     468   98306\n",
       "13157  B_002-0555-1.2.jpg  206  170    309     307   98307\n",
       "13158  B_002-5254-1.2.jpg  375  395    269     265   98308\n",
       "13159  B_026-0011-2.0.jpg  387  321    105     108   98309\n",
       "13160  B_002-4903-1.3.jpg  427  327    303     314   98310\n",
       "\n",
       "[13161 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_csv = pd.read_csv('../csv/label.csv')\n",
    "label_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y0010\\anaconda3\\envs\\CV\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 모델 Return\n",
    "VGG16Model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=(img_width,img_height,3), pooling=None)\n",
    "x = GlobalAveragePooling2D()(VGG16Model.output)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "DeepLearning = Model(inputs=VGG16Model.input, outputs=predictions)\n",
    "\n",
    "# learning parameter를 더하여 최종 model compile\n",
    "DeepLearning.compile(optimizer=\n",
    "         SGD(lr=LearningRate, decay=Decay, momentum=0.9, nesterov=True), \n",
    "         loss='categorical_crossentropy',\n",
    "         metrics=['acc']\n",
    ") # 나이를, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online-augmentation 적용 Generator\n",
    "# 1. 이미지를 전부다 불러서 램 (메모리)에 올릴 수 없기 때문\n",
    "# 2. 이미지는 Augmentation을 해주는게 좋아서\n",
    "\n",
    "DATAGEN_TRAIN = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    data_format=\"channels_last\",\n",
    "    validation_split=0.10) # Train / Validation\n",
    "\n",
    "# Online-augmentation 비적용 Generator (Test용)\n",
    "DATAGEN_TEST = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    data_format=\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10255 images belonging to 2 classes.\n",
      "Found 1138 images belonging to 2 classes.\n",
      "Found 1166 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generator의 instance 생성 (Train)\n",
    "TRAIN_GENERATOR = DATAGEN_TRAIN.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode= \"categorical\",\n",
    "    subset = \"training\")\n",
    "\n",
    "VALID_GENERATOR = DATAGEN_TRAIN.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode= \"categorical\",\n",
    "    subset = \"validation\")\n",
    "\n",
    "# Generator의 instance 생성 (Test)\n",
    "TEST_GENERATOR = DATAGEN_TEST.flow_from_directory(\n",
    "    test_directory,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    class_mode= \"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call-back 함수\n",
    "\n",
    "# CheckPoint: Epoch 마다 validation 성능을 검증하여, best performance 일 경우 저장\n",
    "CP = ModelCheckpoint(filepath=model_directory+'VGG16-{epoch:03d}-{val_loss:.4f}-{val_acc:.4f}.hdf5',\n",
    "            monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# 학습과정 진행사항 확인\n",
    "TB = TensorBoard(log_dir=tensorboard_directory, write_graph=True, write_images=True)\n",
    "\n",
    "# Learning Rate 줄여나가기\n",
    "LR = ReduceLROnPlateau(monitor='val_loss',factor=0.8,patience=3, verbose=1, min_lr=1e-8)\n",
    "\n",
    "CALLBACK = [CP, TB, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y0010\\anaconda3\\envs\\CV\\lib\\site-packages\\keras\\preprocessing\\image.py:1663: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\y0010\\anaconda3\\envs\\CV\\lib\\site-packages\\keras\\preprocessing\\image.py:1671: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    }
   ],
   "source": [
    "########## Training Start\n",
    "DeepLearning.fit(\n",
    "        TRAIN_GENERATOR,\n",
    "        # 데이터가 너무 클 경우 1-epoch을 못하는 경우\n",
    "        # steps_per_epoch=TRAIN_GENERATOR.n / batch_size,\n",
    "        \n",
    "        epochs=15,\n",
    "        callbacks=CALLBACK,\n",
    "        shuffle=True, # Training에 패턴이 존재하면 overfit이 잘 되기 때문에, Shuffle 사용해야함. 단 test에는 절대 X\n",
    "        validation_data=VALID_GENERATOR)\n",
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "85c2a86aa17e3aaf2345d2fecd92be87f1883880644883318c3c0591468b0cd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
