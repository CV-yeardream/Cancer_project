{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12277355831641758072\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3667263488\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1733644973881788180\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import scipy as sp\n",
    "import PIL\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras import models, layers, Model\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, ZeroPadding2D\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import EfficientNetB4, EfficientNetB6, ResNet50V2\n",
    "#from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size: 한번에 forward & Backword 하는 샘플의 수\n",
    "batch_size = 32\n",
    "\n",
    "# Training 수\n",
    "epochs = 50\n",
    "\n",
    "# Weight 조절 parameter\n",
    "LearningRate = 1e-3 # 0.001\n",
    "Decay = 1e-6\n",
    "\n",
    "img_width = 224\n",
    "img_height = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리 경로 설정 필요\n",
    "CurrentDirectory = \"../\"\n",
    "\n",
    "train_directory = CurrentDirectory + 'data/TRAIN/'\n",
    "test_directory  = CurrentDirectory + 'data/TEST/'\n",
    "model_directory = CurrentDirectory + 'MODEL/'\n",
    "tensorboard_directory = CurrentDirectory + 'Tensorboard'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80134624/80134624 [==============================] - 43s 1us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\y0010\\anaconda3\\envs\\CV2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 모델 Return\n",
    "VGG19Model = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_tensor=None, input_shape=(img_width,img_height,3), pooling=None)\n",
    "x = GlobalAveragePooling2D()(VGG19Model.output)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "DeepLearning = Model(inputs=VGG19Model.input, outputs=predictions)\n",
    "\n",
    "# learning parameter를 더하여 최종 model compile\n",
    "DeepLearning.compile(optimizer=\n",
    "         SGD(lr=LearningRate, decay=Decay, momentum=0.9, nesterov=True), \n",
    "         loss='categorical_crossentropy',\n",
    "         metrics=['acc']\n",
    ") # 나이를, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online-augmentation 적용 Generator\n",
    "# 1. 이미지를 전부다 불러서 램 (메모리)에 올릴 수 없기 때문\n",
    "# 2. 이미지는 Augmentation을 해주는게 좋아서\n",
    "\n",
    "DATAGEN_TRAIN = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    data_format=\"channels_last\",\n",
    "    validation_split=0.10) # Train / Validation\n",
    "\n",
    "# Online-augmentation 비적용 Generator (Test용)\n",
    "DATAGEN_TEST = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    data_format=\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10255 images belonging to 2 classes.\n",
      "Found 1138 images belonging to 2 classes.\n",
      "Found 1166 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generator의 instance 생성 (Train)\n",
    "TRAIN_GENERATOR = DATAGEN_TRAIN.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode= \"categorical\",\n",
    "    subset = \"training\")\n",
    "\n",
    "VALID_GENERATOR = DATAGEN_TRAIN.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode= \"categorical\",\n",
    "    subset = \"validation\")\n",
    "\n",
    "# Generator의 instance 생성 (Test)\n",
    "TEST_GENERATOR = DATAGEN_TEST.flow_from_directory(\n",
    "    test_directory,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    class_mode= \"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call-back 함수\n",
    "\n",
    "# CheckPoint: Epoch 마다 validation 성능을 검증하여, best performance 일 경우 저장\n",
    "CP = ModelCheckpoint(filepath=model_directory+'VGG16-{epoch:03d}-{val_loss:.4f}-{val_acc:.4f}.hdf5',\n",
    "            monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# 학습과정 진행사항 확인\n",
    "TB = TensorBoard(log_dir=tensorboard_directory, write_graph=True, write_images=True)\n",
    "\n",
    "# Learning Rate 줄여나가기\n",
    "LR = ReduceLROnPlateau(monitor='val_loss',factor=0.8,patience=3, verbose=1, min_lr=1e-8)\n",
    "\n",
    "CALLBACK = [CP, TB, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\y0010\\anaconda3\\envs\\CV2\\lib\\site-packages\\keras\\preprocessing\\image.py:1663: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "c:\\Users\\y0010\\anaconda3\\envs\\CV2\\lib\\site-packages\\keras\\preprocessing\\image.py:1671: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.5003 - acc: 0.7931\n",
      "Epoch 1: val_acc improved from -inf to 0.79086, saving model to ../MODEL\\VGG16-001-0.5102-0.7909.hdf5\n",
      "321/321 [==============================] - 171s 527ms/step - loss: 0.5003 - acc: 0.7931 - val_loss: 0.5102 - val_acc: 0.7909 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.4661 - acc: 0.8060\n",
      "Epoch 2: val_acc did not improve from 0.79086\n",
      "321/321 [==============================] - 165s 513ms/step - loss: 0.4661 - acc: 0.8060 - val_loss: 0.5108 - val_acc: 0.7909 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.4331 - acc: 0.8196\n",
      "Epoch 3: val_acc did not improve from 0.79086\n",
      "321/321 [==============================] - 166s 517ms/step - loss: 0.4331 - acc: 0.8196 - val_loss: 0.5440 - val_acc: 0.7496 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.4029 - acc: 0.8307\n",
      "Epoch 4: val_acc improved from 0.79086 to 0.79701, saving model to ../MODEL\\VGG16-004-0.5475-0.7970.hdf5\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "321/321 [==============================] - 168s 523ms/step - loss: 0.4029 - acc: 0.8307 - val_loss: 0.5475 - val_acc: 0.7970 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.3731 - acc: 0.8460\n",
      "Epoch 5: val_acc did not improve from 0.79701\n",
      "321/321 [==============================] - 168s 522ms/step - loss: 0.3731 - acc: 0.8460 - val_loss: 0.5319 - val_acc: 0.7610 - lr: 8.0000e-04\n",
      "Epoch 6/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.3656 - acc: 0.8495\n",
      "Epoch 6: val_acc did not improve from 0.79701\n",
      "321/321 [==============================] - 172s 534ms/step - loss: 0.3656 - acc: 0.8495 - val_loss: 0.4961 - val_acc: 0.7803 - lr: 8.0000e-04\n",
      "Epoch 7/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.3533 - acc: 0.8548\n",
      "Epoch 7: val_acc improved from 0.79701 to 0.81195, saving model to ../MODEL\\VGG16-007-0.5085-0.8120.hdf5\n",
      "321/321 [==============================] - 177s 551ms/step - loss: 0.3533 - acc: 0.8548 - val_loss: 0.5085 - val_acc: 0.8120 - lr: 8.0000e-04\n",
      "Epoch 8/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.3427 - acc: 0.8594\n",
      "Epoch 8: val_acc did not improve from 0.81195\n",
      "321/321 [==============================] - 181s 563ms/step - loss: 0.3427 - acc: 0.8594 - val_loss: 0.4927 - val_acc: 0.8111 - lr: 8.0000e-04\n",
      "Epoch 9/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.3385 - acc: 0.8612\n",
      "Epoch 9: val_acc did not improve from 0.81195\n",
      "321/321 [==============================] - 179s 557ms/step - loss: 0.3385 - acc: 0.8612 - val_loss: 0.4960 - val_acc: 0.7794 - lr: 8.0000e-04\n",
      "Epoch 10/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.3292 - acc: 0.8649\n",
      "Epoch 10: val_acc improved from 0.81195 to 0.81722, saving model to ../MODEL\\VGG16-010-0.4609-0.8172.hdf5\n",
      "321/321 [==============================] - 182s 568ms/step - loss: 0.3292 - acc: 0.8649 - val_loss: 0.4609 - val_acc: 0.8172 - lr: 8.0000e-04\n",
      "Epoch 11/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.3243 - acc: 0.8656\n",
      "Epoch 11: val_acc did not improve from 0.81722\n",
      "321/321 [==============================] - 188s 583ms/step - loss: 0.3243 - acc: 0.8656 - val_loss: 0.4535 - val_acc: 0.8137 - lr: 8.0000e-04\n",
      "Epoch 12/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.3188 - acc: 0.8717\n",
      "Epoch 12: val_acc did not improve from 0.81722\n",
      "321/321 [==============================] - 179s 556ms/step - loss: 0.3188 - acc: 0.8717 - val_loss: 0.4614 - val_acc: 0.8058 - lr: 8.0000e-04\n",
      "Epoch 13/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.3122 - acc: 0.8708\n",
      "Epoch 13: val_acc did not improve from 0.81722\n",
      "321/321 [==============================] - 179s 557ms/step - loss: 0.3122 - acc: 0.8708 - val_loss: 0.4677 - val_acc: 0.8014 - lr: 8.0000e-04\n",
      "Epoch 14/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.3062 - acc: 0.8779\n",
      "Epoch 14: val_acc did not improve from 0.81722\n",
      "321/321 [==============================] - 176s 548ms/step - loss: 0.3062 - acc: 0.8779 - val_loss: 0.4420 - val_acc: 0.8172 - lr: 8.0000e-04\n",
      "Epoch 15/15\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.3034 - acc: 0.8743\n",
      "Epoch 15: val_acc did not improve from 0.81722\n",
      "321/321 [==============================] - 176s 547ms/step - loss: 0.3034 - acc: 0.8743 - val_loss: 0.4590 - val_acc: 0.8155 - lr: 8.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x234c613e610>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## Training Start\n",
    "DeepLearning.fit(\n",
    "        TRAIN_GENERATOR,\n",
    "        # 데이터가 너무 클 경우 1-epoch을 못하는 경우\n",
    "        # steps_per_epoch=TRAIN_GENERATOR.n / batch_size,\n",
    "        \n",
    "        epochs=15,\n",
    "        callbacks=CALLBACK,\n",
    "        shuffle=True, # Training에 패턴이 존재하면 overfit이 잘 되기 때문에, Shuffle 사용해야함. 단 test에는 절대 X\n",
    "        validation_data=VALID_GENERATOR)\n",
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('CV2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdcd5b0206629939f51d5066d17712903ce8e88d008384575a999527a8ab4910"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
